{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6-KziCWhBl_",
        "outputId": "71a93fa7-f437-4325-95b2-68948661f7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing is evolving rapidly. It's widely used in chatbots, search engines, and voice assistants. However, tokenization isn't always straightforward.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'evolving',\n",
              " 'rapidly.',\n",
              " \"It's\",\n",
              " 'widely',\n",
              " 'used',\n",
              " 'in',\n",
              " 'chatbots,',\n",
              " 'search',\n",
              " 'engines,',\n",
              " 'and',\n",
              " 'voice',\n",
              " 'assistants.',\n",
              " 'However,',\n",
              " 'tokenization',\n",
              " \"isn't\",\n",
              " 'always',\n",
              " 'straightforward.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "paragraph = (\n",
        "    \"Natural language processing is evolving rapidly. \"\n",
        "    \"It's widely used in chatbots, search engines, and voice assistants. \"\n",
        "    \"However, tokenization isn't always straightforward.\"\n",
        ")\n",
        "\n",
        "print(paragraph)\n",
        "naive_tokens = paragraph.split()\n",
        "naive_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download ALL required punkt resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "paragraph = (\n",
        "    \"Natural language processing is evolving rapidly. \"\n",
        "    \"It's widely used in chatbots, search engines, and voice assistants. \"\n",
        "    \"However, tokenization isn't always straightforward.\"\n",
        ")\n",
        "\n",
        "tool_tokens = word_tokenize(paragraph)\n",
        "tool_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33mi_S73wQR_",
        "outputId": "0f51a1b7-922b-41b8-9102-16f17992aa1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'evolving',\n",
              " 'rapidly',\n",
              " '.',\n",
              " 'It',\n",
              " \"'s\",\n",
              " 'widely',\n",
              " 'used',\n",
              " 'in',\n",
              " 'chatbots',\n",
              " ',',\n",
              " 'search',\n",
              " 'engines',\n",
              " ',',\n",
              " 'and',\n",
              " 'voice',\n",
              " 'assistants',\n",
              " '.',\n",
              " 'However',\n",
              " ',',\n",
              " 'tokenization',\n",
              " 'is',\n",
              " \"n't\",\n",
              " 'always',\n",
              " 'straightforward',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Multiword Expressions (MWEs)\n",
        "MWEs = [\n",
        "    \"Natural language processing\",\n",
        "    \"New York\",\n",
        "    \"search engine\"\n",
        "]\n",
        "\n",
        "MWEs\n",
        "sentence = (\n",
        "    \"Natural language processing is used in search engines. \"\n",
        "    \"New York is a major technology hub.\"\n",
        ")\n",
        "\n",
        "sentence\n",
        "naive_tokens = sentence.split()\n",
        "naive_tokens\n",
        "# Replace MWEs with underscored versions\n",
        "processed_sentence = sentence\n",
        "\n",
        "for mwe in MWEs:\n",
        "    processed_sentence = processed_sentence.replace(mwe, mwe.replace(\" \", \"_\"))\n",
        "\n",
        "processed_sentence\n",
        "mwe_tokens = processed_sentence.split()\n",
        "mwe_tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBX6q79xyLE3",
        "outputId": "7f47338d-5d7c-4739-fb08-bd2da3115750"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural_language_processing',\n",
              " 'is',\n",
              " 'used',\n",
              " 'in',\n",
              " 'search_engines.',\n",
              " 'New_York',\n",
              " 'is',\n",
              " 'a',\n",
              " 'major',\n",
              " 'technology',\n",
              " 'hub.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reflection = \"\"\"\n",
        "The hardest part of tokenization in my language was handling contractions and punctuation consistently.\n",
        "Even in English, clitics like 's and n't require special rules beyond simple space-based splitting.\n",
        "Compared to English, morphologically rich languages have more complex suffixes and inflections, making tokenization harder.\n",
        "Punctuation increases difficulty because it can represent sentence boundaries, emphasis, or emotion.\n",
        "Multiword expressions further complicate tokenization since multiple words often represent a single semantic unit.\n",
        "Overall, punctuation, morphology, and MWEs significantly increase the complexity of accurate tokenization.\n",
        "\"\"\"\n",
        "\n",
        "print(reflection)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g7S-LZaynYV",
        "outputId": "1bfb17b4-e512-42ce-ed11-0d49e7940434"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hardest part of tokenization in my language was handling contractions and punctuation consistently.\n",
            "Even in English, clitics like 's and n't require special rules beyond simple space-based splitting.\n",
            "Compared to English, morphologically rich languages have more complex suffixes and inflections, making tokenization harder.\n",
            "Punctuation increases difficulty because it can represent sentence boundaries, emphasis, or emotion.\n",
            "Multiword expressions further complicate tokenization since multiple words often represent a single semantic unit.\n",
            "Overall, punctuation, morphology, and MWEs significantly increase the complexity of accurate tokenization.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}